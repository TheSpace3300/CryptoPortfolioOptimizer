import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from keras.src.layers import Bidirectional
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from statsmodels.tsa.arima.model import ARIMA
import pmdarima as pm

from services.fetch_data import fetch_ohlcv
from models.arima_model import arima_forecast
from models.holt_winters import holt_winters_forecast
from models.model_selector import select_best_model_ml
from utils.metrics import rmse
from utils.sharpe import sharpe_ratio
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from sklearn.metrics import mean_absolute_error

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

PAIRS = ['BTC/USDT']
results = {}

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
os.makedirs('plots', exist_ok=True)

for pair in PAIRS:
    print(f"\nüîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ä—ã: {pair}")
    series = fetch_ohlcv(pair)
    series.index = pd.to_datetime(series.index)
    series = series.asfreq('D')

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ pandas Series
data = pd.Series(series)

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
plt.plot(data)
plt.title("–í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥")
plt.show()

# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ (–ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1])
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data.values.reshape(-1, 1))

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
plt.plot(scaled_data)
plt.title("–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥")
plt.show()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º—É [samples, time steps, features]
def create_dataset(data, time_step, forecast_horizon):
    X, y = [], []
    for i in range(len(data) - time_step - forecast_horizon):
        X.append(data[i:(i + time_step), 0])  # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö time_step –∑–Ω–∞—á–µ–Ω–∏–π
        y.append(data[(i + time_step):(i + time_step + forecast_horizon), 0])      # –ó–Ω–∞—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –º—ã —Ö–æ—Ç–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å
    return np.array(X), np.array(y)

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è LSTM
time_step = 60  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
forecast_horizon = 10
X, y = create_dataset(scaled_data, time_step, forecast_horizon)

# –ü–µ—Ä–µ—Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º X –≤ —Ñ–æ—Ä–º—É, –ø–æ–¥—Ö–æ–¥—è—â—É—é –¥–ª—è LSTM: [samples, time steps, features]
X = X.reshape(X.shape[0], X.shape[1], 1)

# –†–∞–∑–¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å LSTM
model = Sequential()
# –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–π LSTM
model.add(Bidirectional(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], 1))))
model.add(Dropout(0.3))
model.add(LSTM(units=64))
model.add(Dropout(0.3))
# –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
model.add(Dense(units=forecast_horizon))
# –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
model.compile(optimizer='adam', loss='mean_squared_error')

# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.legend()
plt.title("–ü–æ—Ç–µ—Ä–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
plt.show()

y_pred = model.predict(X_test)

def rescale_batch(batch, scaler, feature_dim):
    rescaled = []
    for row in batch:
        row_reshaped = np.hstack([row.reshape(-1, 1), np.zeros((row.shape[0], feature_dim - 1))])
        row_rescaled = scaler.inverse_transform(row_reshaped)[:, 0]
        rescaled.append(row_rescaled)
    return np.array(rescaled)

feature_dim = scaled_data.shape[1]
y_pred_rescaled = rescale_batch(y_pred, scaler, feature_dim)
y_test_rescaled = rescale_batch(y_test, scaler, feature_dim)

# –û–±—Ä–∞—Ç–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
#y_pred = scaler.inverse_transform(y_pred)
#y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))

plt.plot(y_test_rescaled[0], label='–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
plt.plot(y_pred_rescaled[0], label='–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', color='red')
plt.title('–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ LSTM')
plt.legend()
plt.show()


### 7. **–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏**
from sklearn.metrics import mean_squared_error, mean_absolute_error
import math

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ (MSE, RMSE, MAE)
mse = mean_squared_error(y_test_rescaled, y_pred)
rmse = math.sqrt(mse)
mae = mean_absolute_error(y_test_rescaled, y_pred)

print(f'MSE: {mse}')
print(f'RMSE: {rmse}')
print(f'MAE: {mae}')

last_data = data.values[-time_step:]
last_data.mean()

last_data_scaled = scaler.transform(last_data.reshape(-1, 1))
X_input = last_data_scaled.reshape(1, time_step, 1)
predicted_scaled = model.predict(X_input)
predicted_values = scaler.inverse_transform(predicted_scaled)
print(f'Predicted Price: {predicted_values}')
